"""
Frame Bridge - Video Processing Module
2ã¤ã®å‹•ç”»ã‚’æœ€é©ãªãƒ•ãƒ¬ãƒ¼ãƒ ã§çµåˆã™ã‚‹ãŸã‚ã®å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import cv2
import numpy as np
from PIL import Image
import tempfile
import os
from skimage.metrics import structural_similarity as ssim
from typing import Tuple, List, Optional, Union
from loguru import logger


class VideoProcessor:
    """å‹•ç”»å‡¦ç†ã‚’è¡Œã†ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        pass
        
    def extract_frames(self, video_path: str, num_frames: int = 20) -> Tuple[Optional[List], Optional[str]]:
        """
        å‹•ç”»ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã™ã‚‹
        
        Args:
            video_path: å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            num_frames: æŠ½å‡ºã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            
        Returns:
            Tuple[ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]
        """
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return None, f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ: {video_path}"
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            if total_frames == 0:
                return None, "å‹•ç”»ã«ãƒ•ãƒ¬ãƒ¼ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
            
            logger.info(f"ğŸ“¹ å‹•ç”»è§£æ: {os.path.basename(video_path)} (ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {total_frames})")
            
            frames = []
            # æœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å«ã‚€ç­‰é–“éš”ã§ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º
            frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)
            
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    # BGR to RGBå¤‰æ›
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frames.append((frame_idx, frame_rgb))
            
            cap.release()
            logger.success(f"âœ… ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå®Œäº†: {len(frames)}ãƒ•ãƒ¬ãƒ¼ãƒ ")
            return frames, None
            
        except Exception as e:
            logger.error(f"ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None, f"ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"

    def calculate_frame_similarity(self, frame1: np.ndarray, frame2: np.ndarray) -> float:
        """
        2ã¤ã®ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            frame1: æ¯”è¼ƒãƒ•ãƒ¬ãƒ¼ãƒ 1
            frame2: æ¯”è¼ƒãƒ•ãƒ¬ãƒ¼ãƒ 2
            
        Returns:
            é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
        try:
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›
            gray1 = cv2.cvtColor(frame1, cv2.COLOR_RGB2GRAY)
            gray2 = cv2.cvtColor(frame2, cv2.COLOR_RGB2GRAY)
            
            # åŒã˜ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
            h, w = min(gray1.shape[0], gray2.shape[0]), min(gray1.shape[1], gray2.shape[1])
            gray1 = cv2.resize(gray1, (w, h))
            gray2 = cv2.resize(gray2, (w, h))
            
            # SSIMï¼ˆæ§‹é€ çš„é¡ä¼¼æ€§æŒ‡æ¨™ï¼‰ã‚’è¨ˆç®—
            similarity = ssim(gray1, gray2)
            return max(0.0, similarity)  # è² ã®å€¤ã‚’0ã«ã‚¯ãƒªãƒƒãƒ—
            
        except Exception as e:
            logger.error(f"é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def find_best_connection_frames(self, video1_path: str, video2_path: str) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], float, Optional[str], Tuple[int, int]]:
        """
        2ã¤ã®å‹•ç”»ã®æ¥ç¶šãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹
        å‹•ç”»1ã®æœ€å¾Œã‹ã‚‰2ã¤ç›®ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¨å‹•ç”»2ã®æœ€åˆã‹ã‚‰2ã¤ç›®ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½¿ç”¨
        
        Args:
            video1_path: å‹•ç”»1ã®ãƒ‘ã‚¹
            video2_path: å‹•ç”»2ã®ãƒ‘ã‚¹
            
        Returns:
            Tuple[ãƒ•ãƒ¬ãƒ¼ãƒ 1, ãƒ•ãƒ¬ãƒ¼ãƒ 2, é¡ä¼¼åº¦, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸, ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹]
        """
        try:
            # å„å‹•ç”»ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º
            frames1, error1 = self.extract_frames(video1_path, 30)
            if error1:
                return None, None, 0.0, error1, (0, 0)
            
            frames2, error2 = self.extract_frames(video2_path, 10)
            if error2:
                return None, None, 0.0, error2, (0, 0)
            
            # å‹•ç”»1ã®å®Ÿéš›ã®æœ€å¾Œã‹ã‚‰2ã¤ç›®ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—
            cap1 = cv2.VideoCapture(video1_path)
            total_frames1 = int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))
            cap1.release()
            
            cap2 = cv2.VideoCapture(video2_path)
            total_frames2 = int(cap2.get(cv2.CAP_PROP_FRAME_COUNT))
            cap2.release()
            
            # å®Ÿéš›ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
            idx1 = total_frames1 - 2  # æœ€å¾Œã‹ã‚‰2ã¤ç›®ï¼ˆ0ãƒ™ãƒ¼ã‚¹ï¼‰
            idx2 = 1  # æœ€åˆã‹ã‚‰2ã¤ç›®ï¼ˆ0ãƒ™ãƒ¼ã‚¹ï¼‰
            
            # è©²å½“ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç›´æ¥å–å¾—
            cap1 = cv2.VideoCapture(video1_path)
            cap1.set(cv2.CAP_PROP_POS_FRAMES, idx1)
            ret1, frame1_bgr = cap1.read()
            cap1.release()
            
            cap2 = cv2.VideoCapture(video2_path)
            cap2.set(cv2.CAP_PROP_POS_FRAMES, idx2)
            ret2, frame2_bgr = cap2.read()
            cap2.release()
            
            if not ret1 or not ret2:
                return None, None, 0.0, "æŒ‡å®šãƒ•ãƒ¬ãƒ¼ãƒ ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ", (0, 0)
            
            # BGR to RGBå¤‰æ›
            frame1 = cv2.cvtColor(frame1_bgr, cv2.COLOR_BGR2RGB)
            frame2 = cv2.cvtColor(frame2_bgr, cv2.COLOR_BGR2RGB)
            
            # é¡ä¼¼åº¦ã‚’è¨ˆç®—
            similarity = self.calculate_frame_similarity(frame1, frame2)
            
            logger.info(f"ğŸ”— å›ºå®šãƒ•ãƒ¬ãƒ¼ãƒ çµåˆ: å‹•ç”»1[{idx1}] (æœ€å¾Œã‹ã‚‰2ã¤ç›®) â†’ å‹•ç”»2[{idx2}] (æœ€åˆã‹ã‚‰2ã¤ç›®)")
            logger.info(f"ğŸ“Š ãƒ•ãƒ¬ãƒ¼ãƒ é¡ä¼¼åº¦: {similarity:.3f}")
            
            return frame1, frame2, similarity, None, (idx1, idx2)
            
        except Exception as e:
            logger.error(f"ãƒ•ãƒ¬ãƒ¼ãƒ æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}")
            return None, None, 0.0, f"ãƒ•ãƒ¬ãƒ¼ãƒ æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {str(e)}", (0, 0)

    def create_merged_video(self, video1_path: str, video2_path: str, cut_frame1: int, cut_frame2: int, output_path: str) -> Tuple[bool, Optional[str]]:
        """
        2ã¤ã®å‹•ç”»ã‚’æŒ‡å®šã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã§çµåˆã™ã‚‹
        
        Args:
            video1_path: å‹•ç”»1ã®ãƒ‘ã‚¹
            video2_path: å‹•ç”»2ã®ãƒ‘ã‚¹
            cut_frame1: å‹•ç”»1ã®ã‚«ãƒƒãƒˆãƒ•ãƒ¬ãƒ¼ãƒ 
            cut_frame2: å‹•ç”»2ã®ã‚«ãƒƒãƒˆãƒ•ãƒ¬ãƒ¼ãƒ 
            output_path: å‡ºåŠ›ãƒ‘ã‚¹
            
        Returns:
            Tuple[æˆåŠŸãƒ•ãƒ©ã‚°, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]
        """
        try:
            # å‹•ç”»1ã‚’èª­ã¿è¾¼ã¿
            cap1 = cv2.VideoCapture(video1_path)
            if not cap1.isOpened():
                return False, "å‹•ç”»1ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ"
            
            # å‹•ç”»2ã‚’èª­ã¿è¾¼ã¿
            cap2 = cv2.VideoCapture(video2_path)
            if not cap2.isOpened():
                cap1.release()
                return False, "å‹•ç”»2ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ"
            
            # å‹•ç”»ã®æƒ…å ±ã‚’å–å¾—
            fps1 = cap1.get(cv2.CAP_PROP_FPS)
            width1 = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))
            height1 = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            logger.info(f"ğŸ¬ å‹•ç”»è¨­å®š: {width1}x{height1}, {fps1}fps")
            
            # å‡ºåŠ›å‹•ç”»ã®è¨­å®šï¼ˆæœ€åˆã®å‹•ç”»ã®è¨­å®šã‚’ä½¿ç”¨ï¼‰
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps1, (width1, height1))
            
            # å‹•ç”»1ã®æœ€åˆã‹ã‚‰cut_frame1ã¾ã§
            frame_count = 0
            while frame_count <= cut_frame1:
                ret, frame = cap1.read()
                if not ret:
                    break
                out.write(frame)
                frame_count += 1
            
            logger.info(f"ğŸ“¹ å‹•ç”»1çµåˆ: {frame_count}ãƒ•ãƒ¬ãƒ¼ãƒ ")
            
            # å‹•ç”»2ã®cut_frame2ã‹ã‚‰æœ€å¾Œã¾ã§
            cap2.set(cv2.CAP_PROP_POS_FRAMES, cut_frame2)
            frame_count2 = 0
            while True:
                ret, frame = cap2.read()
                if not ret:
                    break
                # ã‚µã‚¤ã‚ºã‚’å‹•ç”»1ã«åˆã‚ã›ã‚‹
                if frame.shape[:2] != (height1, width1):
                    frame = cv2.resize(frame, (width1, height1))
                out.write(frame)
                frame_count2 += 1
            
            logger.info(f"ğŸ“¹ å‹•ç”»2çµåˆ: {frame_count2}ãƒ•ãƒ¬ãƒ¼ãƒ ")
            
            # ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
            cap1.release()
            cap2.release()
            out.release()
            
            logger.success(f"âœ… å‹•ç”»çµåˆå®Œäº†: {os.path.basename(output_path)}")
            return True, None
            
        except Exception as e:
            logger.error(f"å‹•ç”»çµåˆã‚¨ãƒ©ãƒ¼: {e}")
            return False, f"å‹•ç”»çµåˆã‚¨ãƒ©ãƒ¼: {str(e)}"

    def save_frame_as_image(self, frame: np.ndarray, filename: str) -> Optional[str]:
        """
        ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”»åƒã¨ã—ã¦ä¿å­˜ã™ã‚‹
        
        Args:
            frame: ä¿å­˜ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ 
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            
        Returns:
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            temp_dir = tempfile.gettempdir()
            file_path = os.path.join(temp_dir, filename)
            
            # PIL Imageã«å¤‰æ›ã—ã¦ä¿å­˜
            pil_image = Image.fromarray(frame)
            pil_image.save(file_path)
            
            logger.debug(f"ğŸ–¼ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒä¿å­˜: {os.path.basename(file_path)}")
            return file_path
            
        except Exception as e:
            logger.error(f"ç”»åƒä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def analyze_video_details(self, video_path: str) -> str:
        """
        å‹•ç”»ã®è©³ç´°æƒ…å ±ã‚’åˆ†æã™ã‚‹
        
        Args:
            video_path: å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            å‹•ç”»æƒ…å ±ã®æ–‡å­—åˆ—
        """
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return "å‹•ç”»ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ"
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            duration = frame_count / fps if fps > 0 else 0
            
            cap.release()
            
            return f"""ğŸ“¹ å‹•ç”»æƒ…å ±:
â€¢ è§£åƒåº¦: {width} x {height}
â€¢ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆ: {fps:.2f} FPS
â€¢ ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {frame_count}
â€¢ å†ç”Ÿæ™‚é–“: {duration:.2f} ç§’
â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(video_path) / (1024*1024):.1f} MB"""
            
        except Exception as e:
            logger.error(f"å‹•ç”»åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return f"å‹•ç”»åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"


class FrameBridge:
    """Frame Bridge ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.processor = VideoProcessor()
    
    def process_video_bridge(self, video1_path: str, video2_path: str) -> Tuple[str, Optional[str], Optional[str], Optional[str], float]:
        """
        2ã¤ã®å‹•ç”»ã‚’åˆ†æã—ã¦æœ€é©ãªçµåˆç‚¹ã‚’è¦‹ã¤ã‘ã€çµåˆã™ã‚‹
        
        Args:
            video1_path: å‹•ç”»1ã®ãƒ‘ã‚¹
            video2_path: å‹•ç”»2ã®ãƒ‘ã‚¹
            
        Returns:
            Tuple[çµæœãƒ†ã‚­ã‚¹ãƒˆ, çµåˆå‹•ç”»ãƒ‘ã‚¹, ãƒ•ãƒ¬ãƒ¼ãƒ 1ãƒ‘ã‚¹, ãƒ•ãƒ¬ãƒ¼ãƒ 2ãƒ‘ã‚¹, é¡ä¼¼åº¦]
        """
        if not video1_path or not video2_path:
            return "2ã¤ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚", None, None, None, 0.0
        
        if not os.path.exists(video1_path) or not os.path.exists(video2_path):
            return "æŒ‡å®šã•ã‚ŒãŸå‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", None, None, None, 0.0
        
        try:
            logger.info("ğŸ” å‹•ç”»åˆ†æé–‹å§‹...")
            
            # æœ€é©ãªæ¥ç¶šãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¦‹ã¤ã‘ã‚‹
            frame1, frame2, similarity, error, indices = self.processor.find_best_connection_frames(video1_path, video2_path)
            
            if error:
                return f"ã‚¨ãƒ©ãƒ¼: {error}", None, None, None, 0.0
            
            logger.success("âœ… æœ€é©ãªæ¥ç¶šç‚¹ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”»åƒã¨ã—ã¦ä¿å­˜
            frame1_path = self.processor.save_frame_as_image(frame1, "connection_frame1.png")
            frame2_path = self.processor.save_frame_as_image(frame2, "connection_frame2.png")
            
            logger.info("ğŸ¬ å‹•ç”»çµåˆé–‹å§‹...")
            
            # çµåˆå‹•ç”»ã‚’ä½œæˆ
            temp_dir = tempfile.gettempdir()
            output_path = os.path.join(temp_dir, "merged_video.mp4")
            
            # æœ€é©ãªãƒ•ãƒ¬ãƒ¼ãƒ ã§çµåˆ
            success, merge_error = self.processor.create_merged_video(
                video1_path, video2_path, indices[0], indices[1], output_path
            )
            
            if not success:
                return f"å‹•ç”»çµåˆã‚¨ãƒ©ãƒ¼: {merge_error}", None, None, None, similarity
            
            # å“è³ªè©•ä¾¡
            quality = self._evaluate_quality(similarity)
            
            result_text = f"""ğŸ¬ å‹•ç”»çµåˆå®Œäº†ï¼

ğŸ“Š åˆ†æçµæœ:
â€¢ ãƒ•ãƒ¬ãƒ¼ãƒ é¡ä¼¼åº¦: {similarity:.3f}
â€¢ æ¥ç¶šå“è³ª: {quality}
â€¢ çµåˆãƒ•ãƒ¬ãƒ¼ãƒ : å‹•ç”»1[{indices[0]}] (æœ€å¾Œã‹ã‚‰2ã¤ç›®) â†’ å‹•ç”»2[{indices[1]}] (æœ€åˆã‹ã‚‰2ã¤ç›®)

ğŸ’¡ çµåˆæƒ…å ±:
â€¢ å‹•ç”»1ã®æœ€å¾Œã‹ã‚‰2ã¤ç›®ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§çµ‚äº†
â€¢ å‹•ç”»2ã®æœ€åˆã‹ã‚‰2ã¤ç›®ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰é–‹å§‹
â€¢ å›ºå®šä½ç½®ã§ã®ç¢ºå®Ÿãªæ¥ç¶šã‚’å®Ÿç¾

ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(output_path)}"""
            
            logger.success("ğŸ‰ å‡¦ç†å®Œäº†")
            return result_text, output_path, frame1_path, frame2_path, similarity
            
        except Exception as e:
            logger.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}", None, None, None, 0.0
    
    def _evaluate_quality(self, similarity: float) -> str:
        """é¡ä¼¼åº¦ã‹ã‚‰å“è³ªã‚’è©•ä¾¡"""
        if similarity > 0.8:
            return "å„ªç§€ ğŸŒŸ"
        elif similarity > 0.6:
            return "è‰¯å¥½ âœ…"
        elif similarity > 0.4:
            return "æ™®é€š âš¡"
        else:
            return "è¦ç¢ºèª âš ï¸"